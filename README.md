**Getting in Tune - Exploring the Effects of Instruction Tuning on Performance for Large Language Models**


With the recent launch of ChatGPT, researchers have launched a myriad of new research supported by its use, whereas the general public has exploredthe near-mystical nature of Learning Language Models. However, the frequently less-than-stellar output from ChatGPT can demystify LLMs for non-expert users. ChatGPT can be messy, unpre009 dictable, inconsistent, and even incorrect - and the system has no way of knowing this. However, as a team of self-described ChatGPT power users, we discovered early on that it is possible to improve your experience with the model. This project aims to do just that - by understanding how Instruction Tuning influences how the model responds to ques016 tions and tasks, we set out to determine the best way to phrase requests to ChatGPT. This is vital in018 formation for anyone that regularly relies on LLMs, but unfortunately, there has been a dearth of pub020 lished literature on the subject. This project is our attempt to contribute to that base of knowledge. This field of research is of particular interest to multiple parties. Natural language processing and artificial intelligence researchers are particularly invested, as their work may focus on improving the performance of LLMs like ChatGPT. With a more recent rise in chatbots and virtual assistants, compa028 nies and organizations that utilize these language029 based applications may be interested in the im030 provement of LLMs through instruction tuning. On the non-expert side, those that work in education may be pointedly interested in the improvement of ChatGPTâ€™s outputs, especially those who wish to improve their own prompts for language-based assignments of exam questions for their students.


